{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import tifffile\n",
    "import numpy as np\n",
    "import os\n",
    "from EmbedSeg.utils.preprocess_data import extract_data, split_train_val, split_train_test, split_train_crops, get_data_properties\n",
    "from EmbedSeg.utils.generate_crops import *\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../../data'\n",
    "project_name = 'bbbc010-2012'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, <b>*.tif</b>-type images and the corresponding masks should be respectively present under <b>images</b> and <b>masks</b>, under directories <b>train</b>, <b>val</b> and <b>test</b>, which can be present at any location on your workstation, pointed to by the variable <i>data_dir</i>. (In order to prepare such instance masks, one could use the Fiji plugin <b>Labkit</b> as detailed <b>[here](https://github.com/juglab/EmbedSeg/wiki/Use-Labkit-to-prepare-instance-masks)</b>). The following would be the desired structure as to how data should be present. \n",
    "\n",
    "<img src=\"../../../static/png/01_dir_structure.png\" width=\"100\"/>\n",
    "\n",
    "If you already have your data available in the above style, please skip to the <b><a href=\"#center\">third</a></b> section of this notebook, where you specify the kind of center to which constitutive pixels of an object should embed. \n",
    "Since for the <b> bbbc010-2012</b> we do not have the data in this format yet, we firstly download the data from an external url in the following cells, next we split this data to create our `train`, `val` and `test` directories. \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images and corresponding masks are downloaded from an external url, specified by `zip_url` to the path specified by the variables `data_dir` and `project_name`. The following structure is generated after executing the `extract_data`, `split_train_test` and `split_train_val` methods below:\n",
    "\n",
    "<img src=\"../../../static/png/02_bbbc010-2012.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded data as ../../../data/bbbc010-2012.zip\n",
      "Unzipped data to ../../../data/bbbc010-2012/download/\n"
     ]
    }
   ],
   "source": [
    "extract_data(\n",
    "    zip_url = 'https://github.com/juglab/EmbedSeg/releases/download/v0.1.0/bbbc010-2012.zip',\n",
    "    data_dir = data_dir,\n",
    "    project_name = project_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into `train`, `val` \\& `test`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the `train`-`test` data partition doesn't exist by itself in the original data, we can execute the following cell to reserve some data as evaluation or test data. Here, we reserve 50 % of the available data for evaluation, as is usually done in literature, with regards to the `bbbc010-2012` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new directory : ../../../data/bbbc010-2012/download/test/images\n",
      "Created new directory : ../../../data/bbbc010-2012/download/test/masks\n",
      "Train-Test Images/Masks saved at ../../../data/bbbc010-2012/download\n"
     ]
    }
   ],
   "source": [
    "split_train_test(\n",
    "    data_dir = data_dir,\n",
    "    project_name = project_name, \n",
    "    train_test_name = 'train',\n",
    "    subset = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this dataset, instead of reserving a small fraction of the train dataset for validation at this stage, we first crop the images and masks in the subsequent code cells, and <b><a href= \"split_val\">later</a></b> reserve some of the generated crops for the purposes of validation. We notice that such a strategy allows better results for `bbbc010-2012` during prediction (because of a small dataset size). Running the next cell simply copies the train and test images and masks to the `$data_dir/$project_name/train/.` and `$data_dir/$project_name/test/.` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new directory : ../../../data/bbbc010-2012/train/images/\n",
      "Created new directory : ../../../data/bbbc010-2012/train/masks/\n",
      "Created new directory : ../../../data/bbbc010-2012/val/images/\n",
      "Created new directory : ../../../data/bbbc010-2012/val/masks/\n",
      "Created new directory : ../../../data/bbbc010-2012/test/images/\n",
      "Created new directory : ../../../data/bbbc010-2012/test/masks/\n",
      "Train-Val-Test Images/Masks copied to ../../../data/bbbc010-2012\n"
     ]
    }
   ],
   "source": [
    "split_train_val(\n",
    "    data_dir = data_dir,\n",
    "    project_name = project_name, \n",
    "    train_val_name = 'train',\n",
    "    subset = 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify desired centre location for spatial embedding of pixels\n",
    "<a id='center'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interior pixels of an object instance can either be embedded at the `medoid`, the `approximate-medoid` or the `centroid`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spatial Embedding Location chosen as : medoid\n"
     ]
    }
   ],
   "source": [
    "center = 'medoid'  # 'medoid', 'approximate-medoid', 'centroid'\n",
    "try:\n",
    "    assert center in {'medoid', 'approximate-medoid', 'centroid'}\n",
    "    print(\"Spatial Embedding Location chosen as : {}\".format(center))\n",
    "except AssertionError as e:\n",
    "    e.args += ('Please specify center as one of : {\"medoid\", \"approximate-medoid\", \"centroid\"}', 42)\n",
    "    raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate some dataset specific properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we will calculate properties of the data such as `min_object_size`, `foreground_weight` etc. <br>\n",
    "We will also specify some properties, for example,  \n",
    "* set `data_properties_dir['one_hot'] = True` in case the instances are encoded in a one-hot style. \n",
    "* set `data_properties_dir['data_type']='16-bit'` if the images are of datatype `unsigned 16 bit` and \n",
    "    `data_properties_dir['data_type']='8-bit'` if the images are of datatype `unsigned 8 bit`.\n",
    "\n",
    "Lastly, we will save the dictionary `data_properties_dir` in a json file, which we will access in the `02-train` and `03-predict` notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 202.58it/s]\n",
      " 14%|█▍        | 7/50 [00:00<00:00, 65.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foreground weight of the `bbbc010-2012` dataset set equal to 10.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 69.62it/s]\n",
      " 50%|█████     | 25/50 [00:00<00:00, 242.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum object size of the `bbbc010-2012` dataset is equal to 491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 50/50 [00:00<00:00, 250.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum evaluation image size of the `bbbc010-2012` dataset set equal to (1024, 1024)\n",
      "Dataset properies of the `bbbc010-2012` dataset is saved to `data_properties.json`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "one_hot = True\n",
    "data_properties_dir = get_data_properties(data_dir, project_name, train_val_name=['train'], \n",
    "                                          test_name=['test'], mode='2d', one_hot=one_hot)\n",
    "\n",
    "data_properties_dir['data_type']='16-bit'\n",
    "\n",
    "with open('data_properties.json', 'w') as outfile:\n",
    "    json.dump(data_properties_dir, outfile)\n",
    "    print(\"Dataset properies of the `{}` dataset is saved to `data_properties.json`\".format(project_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify cropping configuration parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images and the corresponding masks are cropped into patches centred around an object instance, which are pre-saved prior to initiating the training. Note that the cropped images, masks and center-images would be saved at the path specified by `crops_dir` (The parameter `crops_dir` is set to ```./crops``` by default, which creates a directory at the same location as this notebook). Here, `data_subset` defines the directory which is processed. Since we only have `train` images and masks at `$data_dir/$project_name/train`, hence we set `data_subset=train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "crops_dir = './crops'\n",
    "data_subset = 'train' \n",
    "crop_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Crops\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "    The cropped images and masks are saved at the same-location as the example notebooks. <br>\n",
    "    Generating the crops might take a little while!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:25<00:00,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropping of images, instances and centre_images for data_subset = `train` done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "image_dir = os.path.join(data_dir, project_name, data_subset, 'images')\n",
    "instance_dir = os.path.join(data_dir, project_name, data_subset, 'masks')\n",
    "image_names = sorted(glob(os.path.join(image_dir, '*.tif'))) \n",
    "instance_names = sorted(glob(os.path.join(instance_dir, '*.tif')))  \n",
    "for i in tqdm(np.arange(len(image_names))):\n",
    "    if one_hot:\n",
    "        process_one_hot(image_names[i], instance_names[i], os.path.join(crops_dir, project_name), data_subset, crop_size, center, one_hot = one_hot)\n",
    "    else:\n",
    "        process(image_names[i], instance_names[i], os.path.join(crops_dir, project_name), data_subset, crop_size, center, one_hot=one_hot)\n",
    "print(\"Cropping of images, instances and centre_images for data_subset = `{}` done!\".format(data_subset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move a fraction of the generated crops for validation purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we reserve a small fraction (15 \\% by default) of the images, masks and center-images crops for the purpose of validation. \n",
    "<a id=\"later_val\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new directory : ./crops/bbbc010-2012/val/images/\n",
      "Created new directory : ./crops/bbbc010-2012/val/masks/\n",
      "Created new directory : ./crops/bbbc010-2012/val/center-medoid/\n",
      "Val Images/Masks/Center-medoid-image crops saved at ./crops/bbbc010-2012/val\n"
     ]
    }
   ],
   "source": [
    "split_train_crops(project_name = project_name, center = center, crops_dir = crops_dir, subset = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EmbedSegEnv",
   "language": "python",
   "name": "embedsegenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
