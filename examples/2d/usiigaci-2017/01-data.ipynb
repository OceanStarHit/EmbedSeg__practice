{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import tifffile\n",
    "import numpy as np\n",
    "import os\n",
    "from EmbedSeg.utils.preprocess_data import extract_data, split_train_val, get_data_properties\n",
    "from EmbedSeg.utils.generate_crops import *\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../../data'\n",
    "project_name = 'usiigaci-2017'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, <b>*.tif</b>-type images and the corresponding masks should be respectively present under <b>images</b> and <b>masks</b>, under directories <b>train</b>, <b>val</b> and <b>test</b>, which can be present at any location on your workstation, pointed to by the variable <i>data_dir</i>. (In order to prepare such instance masks, one could use the Fiji plugin <b>Labkit</b> as detailed <b>[here](https://github.com/juglab/EmbedSeg/wiki/Use-Labkit-to-prepare-instance-masks)</b>). The following would be the desired structure as to how data should be present. \n",
    "\n",
    "<img src=\"../../../static/png/01_dir_structure.png\" width=\"100\"/>\n",
    "\n",
    "If you already have your data available in the above style, please skip to the <b><a href=\"#center\">third</a></b> section of this notebook, where you specify the kind of center to which constitutive pixels of an object should embed. \n",
    "Since for the <b> usiigaci-2017</b> dataset, we do not have the data in this format yet, we firstly download the data from an external url in the following cells, next we split this data to create our `train`, `val` and `test` directories. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images and corresponding masks are downloaded from an external url, specified by `zip_url` to the path specified by the variables `data_dir` and `project_name`. The following structure is generated after executing the `extract_data` method below:\n",
    "\n",
    "<img src=\"../../../static/png/04_usiigaci-2017.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded data as ../../../data/usiigaci-2017.zip\n",
      "Unzipped data to ../../../data/usiigaci-2017/download/\n"
     ]
    }
   ],
   "source": [
    "extract_data(\n",
    "    zip_url = 'https://github.com/juglab/EmbedSeg/releases/download/v0.1.0/usiigaci-2017.zip',\n",
    "    data_dir = data_dir,\n",
    "    project_name = project_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into `train`, `val` \\& `test`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we would like to reserve a small fraction (15 % by default) of the provided train dataset as validation data. Here, in case you would like to repeat multiple experiments with the same partition, you may continue and press <kbd>Shift</kbd> + <kbd>Enter</kbd> on the next cell - but in case, you would like different partitions each time, please add the `seed` attribute equal to a different integer (For example, \n",
    "```\n",
    "split_train_val(\n",
    "data_dir = data_dir, \n",
    "project_name = project_name, \n",
    "train_val_name = 'train', \n",
    "subset = 0.15, \n",
    "seed = 1000)\n",
    "```\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created new directory : ../../../data/usiigaci-2017/train/images/\n",
      "Created new directory : ../../../data/usiigaci-2017/train/masks/\n",
      "Created new directory : ../../../data/usiigaci-2017/val/images/\n",
      "Created new directory : ../../../data/usiigaci-2017/val/masks/\n",
      "Created new directory : ../../../data/usiigaci-2017/test/images/\n",
      "Created new directory : ../../../data/usiigaci-2017/test/masks/\n",
      "Train-Val-Test Images/Masks copied to ../../../data/usiigaci-2017\n"
     ]
    }
   ],
   "source": [
    "split_train_val(\n",
    "    data_dir = data_dir,\n",
    "    project_name = project_name, \n",
    "    train_val_name = 'train',\n",
    "    subset = 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify desired centre location for spatial embedding of pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interior pixels of an object instance can either be embedded at the `medoid`, the `approximate-medoid` or the `centroid`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spatial Embedding Location chosen as : medoid\n"
     ]
    }
   ],
   "source": [
    "center = 'medoid' # 'medoid', 'approximate-medoid', 'centroid'\n",
    "try:\n",
    "    assert center in {'medoid', 'approximate-medoid', 'centroid'}\n",
    "    print(\"Spatial Embedding Location chosen as : {}\".format(center))\n",
    "except AssertionError as e:\n",
    "    e.args += ('Please specify center as one of : {\"medoid\", \"approximate-medoid\", \"centroid\"}', 42)\n",
    "    raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate some dataset specific properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we will calculate properties of the data such as `min_object_size`, `foreground_weight` etc. <br>\n",
    "We will also specify some properties, for example,  \n",
    "* set `data_properties_dir['one_hot'] = True` in case the instances are encoded in a one-hot style. \n",
    "* set `data_properties_dir['data_type']='16-bit'` if the images are of datatype `unsigned 16 bit` and \n",
    "    `data_properties_dir['data_type']='8-bit'` if the images are of datatype `unsigned 8 bit`.\n",
    "\n",
    "Lastly, we will save the dictionary `data_properties_dir` in a json file, which we will access in the `02-train` and `03-predict` notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:00<00:00, 857.83it/s]\n",
      "  0%|          | 0/39 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foreground weight of the `usiigaci-2017` dataset set equal to 10.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:08<00:00,  4.56it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]TiffPage 0: TypeError: read_bytes() missing 3 required positional arguments: 'dtype', 'count', and 'offsetsize'\n",
      "100%|██████████| 5/5 [00:00<00:00, 797.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum object size of the `usiigaci-2017` dataset is equal to 2\n",
      "Maximum evaluation image size of the `usiigaci-2017` dataset set equal to (1024, 1024)\n",
      "Dataset properies of the `usiigaci-2017` dataset is saved to `data_properties.json`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "one_hot = False\n",
    "data_properties_dir = get_data_properties(data_dir, project_name, train_val_name=['train'], \n",
    "                                          test_name=['test'], mode='2d', one_hot=one_hot)\n",
    "\n",
    "data_properties_dir['data_type']='16-bit'\n",
    "\n",
    "with open('data_properties.json', 'w') as outfile:\n",
    "    json.dump(data_properties_dir, outfile)\n",
    "    print(\"Dataset properies of the `{}` dataset is saved to `data_properties.json`\".format(project_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify cropping configuration parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images and the corresponding masks are cropped into patches centred around an object instance, which are pre-saved prior to initiating the training. Here, `data_subsets` is a list of names of directories which is processed. <br>\n",
    "Note that the cropped images, masks and center-images would be saved at the path specified by `crops_dir` (The parameter `crops_dir` is set to ```./crops``` by default, which creates a directory at the same location as this notebook). Please set `one_hot = True` in case the instances are encoded in a one-hot style. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "crops_dir = 'crops'\n",
    "data_subsets = ['train', 'val'] \n",
    "crop_size = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Crops\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "    The cropped images and masks are saved at the same-location as the example notebooks. <br>\n",
    "    Generating the crops would take a little while!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_subset in data_subsets:\n",
    "    image_dir = os.path.join(data_dir, project_name, data_subset, 'images')\n",
    "    instance_dir = os.path.join(data_dir, project_name, data_subset, 'masks')\n",
    "    image_names = sorted(glob(os.path.join(image_dir, '*.tif'))) \n",
    "    instance_names = sorted(glob(os.path.join(instance_dir, '*.tif')))  \n",
    "    for i in tqdm(np.arange(len(image_names))):\n",
    "        if one_hot:\n",
    "            process_one_hot(image_names[i], instance_names[i], os.path.join(crops_dir, project_name), data_subset, crop_size, center, one_hot = one_hot)\n",
    "        else:\n",
    "            process(image_names[i], instance_names[i], os.path.join(crops_dir, project_name), data_subset, crop_size, center, one_hot=one_hot)\n",
    "    print(\"Cropping of images, instances and centre_images for data_subset = `{}` done!\".format(data_subset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EmbedSegEnv",
   "language": "python",
   "name": "embedsegenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
